{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Spark\n",
    "\n",
    "Spark is an extremely powerful cluster computing framework that is used for data workloads that require more resources than can be provided by a single node. There is a wealth of information available online about Spark, as it is one of the most in demand data engineering tools today.\n",
    "\n",
    "In this lab, you'll install Spark onto your local machine and then do some basic tasks. In reality, running Spark on a laptop is a bit silly, as it's designed for larger workloads. That said, the beauty of Spark is that you can write it on a local machine, and then deploy it to a massive cluster and thus scale your workload seamlessly.\n",
    "\n",
    "You may notice that parts of Spark feels a bit like pandas or SQL. This is because much of Spark's feel has been inherited from those tools. In general Spark is just a little more verbose than pandas, but in return for that verbosity, you get incredible computing power.\n",
    "\n",
    "Before we get started you'll need to pip or pip3 install pyspark:\n",
    "\n",
    "*pip3 install --user pyspark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|hello_zipcoder|\n",
      "+--------------+\n",
      "|         spark|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#freebie code to confirm your spark is running correctly\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.sql(\"select 'spark' as hello_zipcoder \")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we're only running this locally - let's get a really big dataset to work with. Through a long legal battle and countless hours of data cleaning, The Washington Post has gained access to, cleaned, and made publically available 6 years of the DEA's data on opiods in America. This was a big deal when it happened in the summer of 2019 and has lots of current relevance. To get started, go and download the dataset from Kaggle [here](https://www.kaggle.com/paultimothymooney/pain-pills-in-the-usa)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
